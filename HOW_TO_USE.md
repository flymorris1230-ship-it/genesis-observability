# ğŸš€ Genesis Observability - ä½¿ç”¨æŒ‡å—

**å®Œæ•´çš„ä½¿ç”¨æ•™å­¸ï¼Œè®“æ‚¨ç«‹å³é–‹å§‹è¿½è¹¤ LLM ä½¿ç”¨é‡èˆ‡æˆæœ¬**

---

## ğŸ“Š ç³»çµ±è³‡è¨Š

æ‚¨çš„ Genesis Observability ç³»çµ±å·²å®Œå…¨éƒ¨ç½²ä¸¦å¯ä½¿ç”¨ï¼š

| çµ„ä»¶ | URL |
|------|-----|
| **Worker API** | https://obs-edge.flymorris1230.workers.dev |
| **Dashboard** | https://genesis-observability-obs-dashboard.vercel.app |
| **API KEY** | `a590aec22adeab9bb9fcf8ff81ccf790a588a298edeffce3216b317c18f87f9e` |

---

## ğŸ¯ 3 æ­¥é©Ÿé–‹å§‹ä½¿ç”¨

### æ­¥é©Ÿ 1: æ•´åˆåˆ°æ‚¨çš„ LLM æ‡‰ç”¨

åœ¨æ¯æ¬¡èª¿ç”¨ LLM å¾Œï¼Œç™¼é€ä½¿ç”¨æ•¸æ“šåˆ° observability ç³»çµ±ã€‚

### æ­¥é©Ÿ 2: æŸ¥çœ‹ Dashboard

å‰å¾€ Dashboard æŸ¥çœ‹å³æ™‚æ•¸æ“šè¦–è¦ºåŒ–ã€æˆæœ¬åˆ†æã€è¶¨å‹¢åœ–è¡¨ã€‚

### æ­¥é©Ÿ 3: ä½¿ç”¨ API æŸ¥è©¢

ä½¿ç”¨ API ç¨‹å¼åŒ–æŸ¥è©¢ metrics å’Œ costs æ•¸æ“šã€‚

---

## ğŸ’» æ•´åˆç¯„ä¾‹

### Node.js / TypeScript

#### åŸºæœ¬æ•´åˆ

```typescript
import fetch from 'node-fetch';

async function sendToObservability(data: {
  project_id: string;
  model: string;
  provider: string;
  input_tokens: number;
  output_tokens: number;
  latency_ms?: number;
  metadata?: Record<string, any>;
}) {
  const response = await fetch('https://obs-edge.flymorris1230.workers.dev/ingest', {
    method: 'POST',
    headers: {
      'Authorization': 'Bearer a590aec22adeab9bb9fcf8ff81ccf790a588a298edeffce3216b317c18f87f9e',
      'Content-Type': 'application/json'
    },
    body: JSON.stringify(data)
  });

  if (!response.ok) {
    console.error('Failed to send observability data:', await response.text());
  }

  return response.json();
}
```

#### OpenAI æ•´åˆ

```typescript
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

async function chat(prompt: string) {
  const startTime = performance.now();

  // èª¿ç”¨ OpenAI
  const response = await openai.chat.completions.create({
    model: 'gpt-4-turbo',
    messages: [{ role: 'user', content: prompt }]
  });

  // ç™¼é€åˆ° observabilityï¼ˆéé˜»å¡ï¼‰
  sendToObservability({
    project_id: 'my-app',  // æ”¹æˆæ‚¨çš„å°ˆæ¡ˆåç¨±
    model: response.model,
    provider: 'openai',
    input_tokens: response.usage!.prompt_tokens,
    output_tokens: response.usage!.completion_tokens,
    latency_ms: Math.round(performance.now() - startTime),
    metadata: {
      feature: 'chat',
      user_id: 'user-123'
    }
  }).catch(console.error);

  return response.choices[0].message.content;
}

// ä½¿ç”¨ç¯„ä¾‹
chat('Hello, how are you?').then(console.log);
```

#### Claude æ•´åˆ

```typescript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
});

async function chat(prompt: string) {
  const startTime = performance.now();

  const response = await anthropic.messages.create({
    model: 'claude-3-sonnet-20240229',
    max_tokens: 1024,
    messages: [{ role: 'user', content: prompt }]
  });

  // ç™¼é€åˆ° observability
  sendToObservability({
    project_id: 'my-app',
    model: 'claude-3-sonnet',
    provider: 'anthropic',
    input_tokens: response.usage.input_tokens,
    output_tokens: response.usage.output_tokens,
    latency_ms: Math.round(performance.now() - startTime)
  }).catch(console.error);

  return response.content[0].text;
}
```

#### Google Gemini æ•´åˆ

```typescript
import { GoogleGenerativeAI } from '@google/generative-ai';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);

async function chat(prompt: string) {
  const startTime = performance.now();

  const model = genAI.getGenerativeModel({ model: 'gemini-pro' });
  const result = await model.generateContent(prompt);
  const response = await result.response;

  // Gemini å…è²»ç‰ˆä¸æä¾›ç¢ºåˆ‡ token æ•¸ï¼Œéœ€è¦ä¼°ç®—
  const inputTokens = Math.ceil(prompt.length / 4);
  const outputTokens = Math.ceil(response.text().length / 4);

  sendToObservability({
    project_id: 'my-app',
    model: 'gemini-pro',
    provider: 'google',
    input_tokens: inputTokens,
    output_tokens: outputTokens,
    latency_ms: Math.round(performance.now() - startTime)
  }).catch(console.error);

  return response.text();
}
```

---

### Python

#### åŸºæœ¬æ•´åˆ

```python
import requests
import time
from typing import Dict, Any, Optional

def send_to_observability(
    project_id: str,
    model: str,
    provider: str,
    input_tokens: int,
    output_tokens: int,
    latency_ms: Optional[int] = None,
    metadata: Optional[Dict[str, Any]] = None
):
    response = requests.post(
        'https://obs-edge.flymorris1230.workers.dev/ingest',
        headers={
            'Authorization': 'Bearer a590aec22adeab9bb9fcf8ff81ccf790a588a298edeffce3216b317c18f87f9e',
            'Content-Type': 'application/json'
        },
        json={
            'project_id': project_id,
            'model': model,
            'provider': provider,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'latency_ms': latency_ms,
            'metadata': metadata or {}
        }
    )

    if not response.ok:
        print(f'Failed to send observability data: {response.text}')

    return response.json()
```

#### OpenAI æ•´åˆ

```python
import openai
import os

openai.api_key = os.getenv('OPENAI_API_KEY')

def chat(prompt: str) -> str:
    start_time = time.time()

    response = openai.ChatCompletion.create(
        model='gpt-4-turbo',
        messages=[{'role': 'user', 'content': prompt}]
    )

    latency = int((time.time() - start_time) * 1000)

    # ç™¼é€åˆ° observability
    send_to_observability(
        project_id='my-app',
        model=response.model,
        provider='openai',
        input_tokens=response.usage.prompt_tokens,
        output_tokens=response.usage.completion_tokens,
        latency_ms=latency,
        metadata={
            'feature': 'chat',
            'user_id': 'user-123'
        }
    )

    return response.choices[0].message.content

# ä½¿ç”¨ç¯„ä¾‹
result = chat('Hello, how are you?')
print(result)
```

#### Claude æ•´åˆ

```python
import anthropic

client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

def chat(prompt: str) -> str:
    start_time = time.time()

    message = client.messages.create(
        model='claude-3-sonnet-20240229',
        max_tokens=1024,
        messages=[{'role': 'user', 'content': prompt}]
    )

    send_to_observability(
        project_id='my-app',
        model='claude-3-sonnet',
        provider='anthropic',
        input_tokens=message.usage.input_tokens,
        output_tokens=message.usage.output_tokens,
        latency_ms=int((time.time() - start_time) * 1000)
    )

    return message.content[0].text
```

---

### Go

```go
package main

import (
    "bytes"
    "encoding/json"
    "fmt"
    "net/http"
    "time"
)

type ObservabilityData struct {
    ProjectID    string                 `json:"project_id"`
    Model        string                 `json:"model"`
    Provider     string                 `json:"provider"`
    InputTokens  int                    `json:"input_tokens"`
    OutputTokens int                    `json:"output_tokens"`
    LatencyMs    int                    `json:"latency_ms,omitempty"`
    Metadata     map[string]interface{} `json:"metadata,omitempty"`
}

func SendToObservability(data ObservabilityData) error {
    jsonData, err := json.Marshal(data)
    if err != nil {
        return err
    }

    req, err := http.NewRequest("POST",
        "https://obs-edge.flymorris1230.workers.dev/ingest",
        bytes.NewBuffer(jsonData))
    if err != nil {
        return err
    }

    req.Header.Set("Authorization", "Bearer a590aec22adeab9bb9fcf8ff81ccf790a588a298edeffce3216b317c18f87f9e")
    req.Header.Set("Content-Type", "application/json")

    client := &http.Client{Timeout: 10 * time.Second}
    resp, err := client.Do(req)
    if err != nil {
        return err
    }
    defer resp.Body.Close()

    if resp.StatusCode >= 400 {
        return fmt.Errorf("observability API error: %d", resp.StatusCode)
    }

    return nil
}

// ä½¿ç”¨ç¯„ä¾‹
func main() {
    err := SendToObservability(ObservabilityData{
        ProjectID:    "my-app",
        Model:        "gpt-4",
        Provider:     "openai",
        InputTokens:  1000,
        OutputTokens: 500,
        LatencyMs:    1200,
    })

    if err != nil {
        fmt.Println("Error:", err)
    }
}
```

---

## ğŸ“Š æŸ¥çœ‹æ•¸æ“š

### æ–¹æ³• 1: Dashboardï¼ˆæ¨è–¦ï¼‰

å‰å¾€ï¼šhttps://genesis-observability-obs-dashboard.vercel.app

åŠŸèƒ½ï¼š
- ğŸ“ˆ **å³æ™‚åœ–è¡¨** - Tokens ä½¿ç”¨é‡ã€å»¶é²è¶¨å‹¢
- ğŸ’° **æˆæœ¬åˆ†æ** - æŒ‰ modelã€provider åˆ†çµ„
- ğŸ” **ç¯©é¸å™¨** - æŒ‰ projectã€æ™‚é–“ç¯„åœ
- ğŸ“Š **å½™ç¸½æ•¸æ“š** - ç¸½è«‹æ±‚æ•¸ã€ç¸½æˆæœ¬ã€å¹³å‡å»¶é²

### æ–¹æ³• 2: API æŸ¥è©¢

#### æŸ¥è©¢ Metrics

```bash
curl "https://obs-edge.flymorris1230.workers.dev/metrics?project_id=my-app&start_date=2025-01-01&end_date=2025-12-31" \
  -H "Authorization: Bearer a590aec22adeab9bb9fcf8ff81ccf790a588a298edeffce3216b317c18f87f9e"
```

**å›æ‡‰ç¯„ä¾‹**ï¼š
```json
{
  "project_id": "my-app",
  "metrics": {
    "totalRequests": 150,
    "totalTokens": 225000,
    "totalCost": 6.75,
    "avgLatency": 1150,
    "modelBreakdown": {
      "gpt-4-turbo": {
        "count": 100,
        "tokens": 150000,
        "cost": 4.5
      },
      "claude-3-sonnet": {
        "count": 50,
        "tokens": 75000,
        "cost": 2.25
      }
    }
  }
}
```

#### æŸ¥è©¢ Costs

```bash
curl "https://obs-edge.flymorris1230.workers.dev/costs?project_id=my-app&start_date=2025-01-01&end_date=2025-12-31" \
  -H "Authorization: Bearer a590aec22adeab9bb9fcf8ff81ccf790a588a298edeffce3216b317c18f87f9e"
```

**å›æ‡‰ç¯„ä¾‹**ï¼š
```json
{
  "project_id": "my-app",
  "summary": {
    "totalCost": 6.75,
    "dailyCosts": [
      {"date": "2025-10-07", "cost": 0.45},
      {"date": "2025-10-06", "cost": 0.38}
    ],
    "providerCosts": [
      {"provider": "openai", "cost": 4.5},
      {"provider": "anthropic", "cost": 2.25}
    ]
  }
}
```

---

## ğŸ§ª æ¸¬è©¦

### å¿«é€Ÿæ¸¬è©¦

```bash
# ç™¼é€æ¸¬è©¦æ•¸æ“š
curl -X POST https://obs-edge.flymorris1230.workers.dev/ingest \
  -H "Authorization: Bearer a590aec22adeab9bb9fcf8ff81ccf790a588a298edeffce3216b317c18f87f9e" \
  -H "Content-Type: application/json" \
  -d '{
    "project_id": "test",
    "model": "gpt-4",
    "provider": "openai",
    "input_tokens": 1000,
    "output_tokens": 500,
    "latency_ms": 1200
  }'

# æŸ¥è©¢æ•¸æ“š
curl "https://obs-edge.flymorris1230.workers.dev/metrics?project_id=test" \
  -H "Authorization: Bearer a590aec22adeab9bb9fcf8ff81ccf790a588a298edeffce3216b317c18f87f9e"
```

### é‹è¡Œå®Œæ•´æ¸¬è©¦

```bash
cd /path/to/genesis-observability
./scripts/test-e2e.sh
```

---

## ğŸ’¡ æœ€ä½³å¯¦è¸

### 1. ä½¿ç”¨æœ‰æ„ç¾©çš„ project_id

```typescript
// âŒ ä¸å¥½
project_id: 'app'

// âœ… å¥½
project_id: 'my-chatbot-prod'
project_id: 'customer-support-staging'
```

### 2. æ·»åŠ æœ‰ç”¨çš„ metadata

```typescript
metadata: {
  user_id: 'user-123',
  user_tier: 'premium',
  feature: 'chat',
  conversation_id: 'conv-456',
  environment: 'production',
  version: '1.2.0'
}
```

### 3. éé˜»å¡èª¿ç”¨

```typescript
// âœ… ä½¿ç”¨ .catch() é¿å…å½±éŸ¿ä¸»æµç¨‹
sendToObservability(data).catch(console.error);

// âœ… æˆ–ä½¿ç”¨ Promise.race åŠ ä¸Š timeout
Promise.race([
  sendToObservability(data),
  new Promise((_, reject) => setTimeout(() => reject(new Error('Timeout')), 5000))
]).catch(err => console.error('Observability error:', err));
```

### 4. å€åˆ†ç’°å¢ƒ

```typescript
const project_id = process.env.NODE_ENV === 'production'
  ? 'my-app-prod'
  : 'my-app-dev';
```

---

## ğŸ“š ç›¸é—œæ–‡æª”

- **INTEGRATION_GUIDE.md** - è©³ç´°æ•´åˆæŒ‡å—ï¼ˆæ‰€æœ‰èªè¨€èˆ‡æ¡†æ¶ï¼‰
- **DEPLOYMENT_SUCCESS.md** - éƒ¨ç½²è³‡è¨Šèˆ‡ç³»çµ±ç‹€æ…‹
- **QUICK_START.md** - å¿«é€Ÿé–‹å§‹æŒ‡å—
- **README.md** - å°ˆæ¡ˆç¸½è¦½

---

## ğŸ†˜ å¸¸è¦‹å•é¡Œ

### Q: å¦‚ä½•è¿½è¹¤å¤šå€‹å°ˆæ¡ˆï¼Ÿ

A: ä½¿ç”¨ä¸åŒçš„ `project_id`ï¼š

```typescript
sendToObservability({ project_id: 'chatbot', ... });
sendToObservability({ project_id: 'support', ... });
sendToObservability({ project_id: 'analytics', ... });
```

### Q: å¦‚ä½•æŸ¥çœ‹ç‰¹å®šæ™‚é–“ç¯„åœçš„æ•¸æ“šï¼Ÿ

A: ä½¿ç”¨ `start_date` å’Œ `end_date` åƒæ•¸ï¼š

```bash
curl "https://obs-edge.flymorris1230.workers.dev/metrics?project_id=my-app&start_date=2025-10-01&end_date=2025-10-07" \
  -H "Authorization: Bearer API_KEY"
```

### Q: æˆæœ¬æ˜¯å¦‚ä½•è¨ˆç®—çš„ï¼Ÿ

A: ç³»çµ±æ ¹æ“š model å’Œ tokens è‡ªå‹•è¨ˆç®—æˆæœ¬ã€‚æ‚¨ä¹Ÿå¯ä»¥æ‰‹å‹•æŒ‡å®š `cost_usd`ï¼š

```typescript
sendToObservability({
  // ...
  cost_usd: 0.045  // æ‰‹å‹•æŒ‡å®šæˆæœ¬
});
```

### Q: å¦‚ä½•ç¢ºä¿æ•¸æ“šéš±ç§ï¼Ÿ

A:
- API KEY åƒ…åœ¨ä¼ºæœå™¨ç«¯ä½¿ç”¨
- ä¸è¦åœ¨ `metadata` ä¸­åŒ…å«æ•æ„Ÿè³‡è¨Š
- ä½¿ç”¨ç’°å¢ƒè®Šæ•¸å„²å­˜ API KEY

---

## ğŸ‰ é–‹å§‹è¿½è¹¤ï¼

ç¾åœ¨æ‚¨å·²ç¶“äº†è§£å¦‚ä½•ä½¿ç”¨ Genesis Observabilityï¼

**ä¸‹ä¸€æ­¥**ï¼š
1. æ•´åˆåˆ°æ‚¨çš„ LLM æ‡‰ç”¨
2. å‰å¾€ Dashboard æŸ¥çœ‹æ•¸æ“š
3. é–‹å§‹å„ªåŒ–æˆæœ¬èˆ‡æ€§èƒ½

**Dashboard**: https://genesis-observability-obs-dashboard.vercel.app

ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼ğŸš€
